{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6\n",
    "\n",
    "Gordon Richards (2016, 2018, 2020, 2022)\n",
    "\n",
    "In Chapter 6 we will cover the following topics\n",
    "* Non-parametric Density Estimation, specifically Kernel Density Estimation (KDE)\n",
    "* $k$-Nearest Neighbor Density Estimation\n",
    "* Parametric Density Estimation, specifically Gaussian Mixture Models (GMM)\n",
    "* Clustering algorithms, particularly $K$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "* [Density Estimation](#one)\n",
    "* [Bandwidth Determination](#two)\n",
    "* [2-D Histograms](#three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Density Estimation <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "Inferring the pdf of a sample of data is known as **density estimation**.  Essentially we are smoothing the data to correct for the finiteness of our sample and to better recover the underlying distribution.\n",
    "\n",
    "Density estimation is useful because:\n",
    "- identifying low probability regions can help uncover rare sources. \n",
    "- if the data can be divided into sub-samples, one can estimate the pdf for each subsample and, in turn determine classifications for new objects.\n",
    "\n",
    "*Nonparametric* density estimation is useful when we know nothing about the underlying distribution of the data since we don't have to specify a model.  This flexibility allows us to capture the shape of the distribution well, at the expense of more difficulty interpreting the results.\n",
    "\n",
    "[*Kernel Density Estimation (KDE)*](https://en.wikipedia.org/wiki/Kernel_density_estimation) is the standard here (and, incidentally, is something that we have been doing in my group for about 15 years now).\n",
    "\n",
    "Let's start by recalling the experiment that we did with 1-D histograms in the first week of class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# From Ivezic, Figure 6.1, modified by GTR\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Draw the random data\n",
    "np.random.seed(1)\n",
    "x = np.concatenate([np.random.normal(-0.5, 0.3, size=14), np.random.normal(1, 0.3, size=7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# First figure: silly histogram binning\n",
    "fig1 = plt.figure(figsize=(8, 4))\n",
    "fig1.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.15, top=0.9, hspace=0.05)\n",
    "\n",
    "FC = '#6666FF'\n",
    "XLIM = (-2, 2.9)\n",
    "YLIM = (-0.09, 1.1)\n",
    "\n",
    "ax = fig1.add_subplot(121)\n",
    "bins = np.linspace(-1.8, 2.7, 13)\n",
    "ax.hist(x, bins=bins, density=True, histtype='stepfilled', fc='k', alpha=0.3)\n",
    "ax.plot(XLIM, [0, 0], '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "#Shift bin centers by 0.25\n",
    "ax = fig1.add_subplot(122)\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.hist(x, bins=bins + 0.25, density=True, histtype='stepfilled', fc='k', alpha=0.3)\n",
    "ax.plot(XLIM, [0, 0], '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The underlying distribution in both panels is the same, that is the data points that make up the histogram are the same.  All we have done is shifted the locations of the bins by 0.25.\n",
    "As we saw in Lecture 2, the choice of bin centers can really change the histogram that we make.\n",
    "\n",
    "The next panels are what happens if we center the bins on each point.  This is an example of **kernel density estimation (KDE)** using a \"top-hat\" kernel.  It is a good description of the data, but is pretty ugly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig1b = plt.figure(figsize=(8, 4))\n",
    "fig1b.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.1, top=0.95, hspace=0.05)\n",
    "\n",
    "ax = fig1b.add_subplot(111)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = (abs(x_plot - x[:, None]) <= 0.5 * binwidth).astype(float)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_ylabel('$p(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can make it look nicer by choosing a different kernel.  That is by choosing a different bin shape.  The next 3 plots show KDEs using **Gaussian kernels** with different width Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# First figure: transition to KDE\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "fig2.subplots_adjust(left=0.12, right=0.95, wspace=0.05, bottom=0.0, top=1.0, hspace=0.05)\n",
    "\n",
    "ax = fig2.add_subplot(311)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.1)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "\n",
    "ax = fig2.add_subplot(312)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.7)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, 4 * y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_ylabel('$p(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "ax = fig2.add_subplot(313)\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], 0.2)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)\n",
    "ax.set_xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can think of KDE as replacing the points with \"clouds\". Each cloud is described by the kernel $K(u)$, where $K(u)$ can be any function that is:\n",
    "- smooth,\n",
    "- postive definite,\n",
    "- normalizes to unity, \n",
    "- has zero mean,\n",
    "- has positive variance. \n",
    "\n",
    "The KDE plots above look better, but give us a \"Goldilocks\" problem.  The first plot uses a kernel that is too narrow.  The second is too wide.  The third is \"just right\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A common kernel is the Gaussian kernel that we just used above:\n",
    "\n",
    "$$K(u) = \\frac{1}{(2\\pi)^{D/2}}\\exp^{-u^2/2}$$\n",
    "\n",
    "Note that the \"$D$\" is necessary because while histograms are generally 1-D, the kind of Big Data analysis that we are interested in will be $N$-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once a kernel is chosen the kernel density estimate at a given point, $x$, is given by \n",
    "$$ \\hat{f}(x) = \\frac{1}{Nh^D}\\sum_{i=1}^N K\\left(\\frac{d(x,x_i)}{h}\\right),$$\n",
    "where $\\hat{f}$ is an **estimator** of our distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where does this come from?  Well if you wanted to know the density of points you could compute\n",
    "$\\frac{\\sum_1^N\\delta (x-x_i)}{V},$ where $\\delta (x-x_i)$ is the Delta function, $V$ is the volume, and $N$ is the number of points.  In $D$-dimensional space a volume element is just $h^D$.  Then instead of representing our observation as a delta function, we represent it by our kernel function.  To normalize for the number of points, divide by $N$.\n",
    "\n",
    "The argument of $K$ is just some measure of the distance between $x$ and each $x_i$.  Normally $d(x,x_i) = (x-x_i)$.  For the gaussian kernel that makes $h=\\sigma$.  Take a second to convince yourself that that is the case.  So, you can see how $h$ represents the \"width\" or what is usually called the **bandwidth** in this context.\n",
    "\n",
    "You might wonder why the book uses $\\hat{f}$ instead of just $f$ since we already are using $f$ instead of $h$ (the true distribution). I don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a comparison of some different possible kernels.  The one that I use most commonly is actually an Epanechnikov kernel since the Gaussian and Exponential have rather long tails.\n",
    "![Ivezic, Figure 6.2](http://www.astroml.org/_images/fig_kernels_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We won't go through the math, but it turns out that the Epanechnikov kernel is \"optimal\" in the sense of minimizing the variance.  That kernel is given by $$K(x) = \\frac{3}{4}(1-x^2),$$\n",
    "for $|x|\\le 1$ and 0 otherwise. Below is the code that produces the plot above.  \n",
    "\n",
    "You are going to add the Epanechnikov kernel to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute Kernels.\n",
    "u = np.linspace(-5, 5, 10000)\n",
    "du = u[1] - u[0]\n",
    "\n",
    "gauss = (1. / np.sqrt(2 * np.pi)) * np.exp(-0.5 * u ** 2)\n",
    "\n",
    "exp = 0.5 * np.exp(-abs(u))\n",
    "\n",
    "tophat = 0.5 * np.ones_like(u)\n",
    "tophat[abs(u) > 1] = 0 # Range of the tophat kernel\n",
    "\n",
    "ep = ____  # Add the Epanechnikov kernel function\n",
    "ep[____]=0 # Set the range of the kernel\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the kernels\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(u, gauss, '-', c='black', lw=3, label='Gaussian')\n",
    "ax.plot(u, exp, '-', c='#666666', lw=2, label='Exponential')\n",
    "ax.plot(u, tophat, '-', c='#999999', lw=1, label='Top-hat')\n",
    "ax.plot(__,__,__,__,label='Epanechnikov')  # Add the Epanechnikov kernel to the plot\n",
    "\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('$u$')\n",
    "ax.set_ylabel('$K(u)$')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(0, 0.8001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bandwidth Determination <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "When we first discussed histograms and KDE we talked about theoretical computation of optimal bandwidths.  Let's now look at how we can empirically determine the optimal bandwidth through [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).\n",
    "\n",
    "To avoid fooling ourselves into thinking our model is better than it is, we set aside some of the data as the \"***test***\" set--that provides an independent check on the the fit from the ***training data***. \n",
    "\n",
    "That's fine if our model has no parameters, or has parameters that are all fixed ahead of time.  But what if we want to try to use the data to not only fit a model, but decide what the best model parameters are?  In that case, we need to set aside some of the training data as a \"***validation***\" set.  \n",
    "\n",
    "However, not only does that leave us even less data for training, but it also means that it matters which objects are in each of the training, validation, and test sets.  We can solve both of these problems with **cross validation**.\n",
    "\n",
    "With our KDE example, we are just trying to map the density, not predict the value, so we really only need two sets: training and validation. There are a number of different ways to do this.\n",
    "\n",
    "1. You could **randomly sample** to decide which data goes into the training or test sets. We don't just do this once, but rather many times so that each data point is treated both as a training point and as a test point at some stage.\n",
    "\n",
    "\n",
    "2. We could do this in a more ordered way (e.g., to make sure that each object gets sampled as training/test the same number of times) and do a **$K$-fold cross validation**. Here $K$ is the number of \"experiments\" that need to be done so that each data point appears in a test sample once.\n",
    "\n",
    "\n",
    "3. We can take $K$-fold cross validation to the extreme by having $K\\equiv N$, so that in each experiment we leave out just one object. This is called **\"Leave-One-Out\" cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustration of Random Sampling\n",
    "![Random Sample Cross Validation Example; Remesan Figure 3.7](http://i.stack.imgur.com/4Lrff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustration of K-Fold Cross Validation\n",
    "![K-Fold Cross Validation Example; Remesan Figure 3.8](http://i.stack.imgur.com/fhMza.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustration of Leave-One-Out Cross Validation\n",
    "![Leave-One-Out Cross Validation Example; Remesan Figure 3.9](http://images.slideplayer.com/16/4977882/slides/slide_35.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's go back to our problem of trying to determine the best bandwidth for our Kernel Density Estimate.  We can do this in Scikit-Learn with [`GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) and replot our histogram above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Complete and Execute this cell to determine the bandwidth\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "bwrange = np.linspace(____, ____, ____) # Test 30 bandwidths from 0.1 to 1.0\n",
    "K = ____ # Do 5-fold cross validation\n",
    "grid = GridSearchCV(KernelDensity(), {'bandwidth': ____}, cv=____) # Try each bandwidth with K-folds\n",
    "grid.fit(x[:, None]) #Fit the histogram data that we started the lecture with.\n",
    "h_opt = grid.best_params_['bandwidth']\n",
    "print(h_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to see the new \"histogram\"\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "ax = fig2.add_subplot(111)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "binwidth = bins[1] - bins[0]\n",
    "x_plot = np.linspace(-4, 4, 1000)\n",
    "y_plot = binwidth * stats.norm.pdf(x_plot, x[:, None], h_opt)\n",
    "y_plot /= (binwidth * len(x))\n",
    "ax.fill(x_plot, y_plot.sum(0), ec='k', lw=1, fc='k', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot.T, '-k', lw=1)\n",
    "ax.plot(x, 0 * x - 0.05, '+k')\n",
    "ax.set_xlim(XLIM)\n",
    "ax.set_ylim(YLIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2-D Histograms <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Here is some sample code using [`sklearn.neighbors.KernelDensity`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html).  Play around with this and see how it works.  Make different variations of the plot.  (For example, try `bandwidth=0.01` and `bandwidth=1.0`.)  What we are doing here is using KDE to set the plot color to indicate the relative density of the points.  This is essentially a 2-D histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Two 2-D normal distributions with offset centroids\n",
    "# See what happens when you make changes to the next 2 lines.\n",
    "X = np.concatenate([np.random.normal([-1,-1],[0.75,0.75],size=(1000,2)),np.random.normal([1,1],[1,1],size=(500,2))]) \n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.1)\n",
    "kde.fit(X) #fit the model to the data\n",
    "\n",
    "u = v = np.linspace(-4,5,80)\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(u, v))).T\n",
    "dens = np.exp(kde.score_samples(Xgrid)) #evaluate the model on the grid\n",
    "\n",
    "plt.scatter(Xgrid[:,0],Xgrid[:,1], c=dens, cmap=\"Purples\", edgecolor=\"None\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now copy the example from above to a new cell and splice in the cross validation code to produce a new plot with the \"optimal\" bandwidth.  Basically, splice in the lines of code for `GridSearchCV` between the lines setting `X` and instantiating `kde`, choosing an appropriate range of bandwidths to test.  What is the bandwidth that is chosen?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
